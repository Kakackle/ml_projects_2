{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4378282-ce27-42d0-80e3-5bd7435ee12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\programowanie_web_etc\\python_projects\\ml_projects_2\\ml_basic_pytorch\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "torchvision version: 0.16.2+cu118\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfbf93-eeda-4056-b237-96c77958e15a",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866bc5c2-d496-4f26-b97e-c4361abb4525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962257d-eb71-45ab-8313-56d1b428ec53",
   "metadata": {},
   "source": [
    "#### MNIST download from pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a88e42-756a-424f-aa75-f8885592f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f7b5a9-af3d-4e50-8b3d-a4f0ff974da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b390854-edd7-4973-aa60-5a0c6a5f092e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668e92ba-9ce8-49d7-abd4-21042bdfc20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d53f8c-8337-48ac-8b91-b9ffa4a07225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[99]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87476b1b-fcca-40fa-a81d-10845c97abf9",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa41ba0f-4da9-4fee-a7e6-14505081d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x000001B8AFDFE0B0>, <torch.utils.data.dataloader.DataLoader object at 0x000001B8AFDFE4A0>)\n",
      "Length of train dataloader: 469 batches of 128\n",
      "Length of test dataloader: 79 batches of 128\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955f377b-e514-48e2-9a14-3f10123042ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d2e3-5baf-47db-bb5a-505203c8e1d7",
   "metadata": {},
   "source": [
    "#### Random image from batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1beea5cc-a7e7-4768-9ffd-d2ccf0b9c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 2, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbb9f24-33e3-4938-bc7c-5f9500cbe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionModelV0(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d231401c-e246-4653-bd13-0a6d59740402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionModelV0(\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = FashionModelV0(\n",
    "    input_shape = 28*28,\n",
    "    hidden_units = 100,\n",
    "    output_shape=len(class_names)\n",
    ")\n",
    "model_0.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836126e3-f689-4325-b552-ef24a8402e21",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "930312ae-d63a-4096-aa02-23e187f249ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0063d21b-679e-459c-a287-e73fb0b5681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d21621-4bd0-46e2-b0c2-ed819e6e818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73dfa8-523c-4958-b510-30916ed4f476",
   "metadata": {},
   "source": [
    "### Creating a function to time our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca31781-8d9c-4015-987b-293b2e9b1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b26439-84ad-4d83-aeb5-8872b5653a97",
   "metadata": {},
   "source": [
    "### Functionized training and testing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ccff60-a953-459d-80ec-03e7051b86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a06c2e-4013-4f85-91fa-a43a96933a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                       | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 1.04816 | Train accuracy: 67.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████▋                                                                                               | 1/3 [00:04<00:08,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.76194 | Test accuracy: 75.00%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.69971 | Train accuracy: 76.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 2/3 [00:08<00:04,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.71219 | Test accuracy: 76.29%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.64949 | Train accuracy: 78.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.68493 | Test accuracy: 77.23%\n",
      "\n",
      "Train time on cuda: 13.407 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_0, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_0,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436e97e-a7e7-4ea1-af3b-36e850ce6002",
   "metadata": {},
   "source": [
    "### Function to evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e833c1-3956-41ad-83c5-786199d41e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionModelV0',\n",
       " 'model_loss': 0.6849288940429688,\n",
       " 'model_acc': 77.22507911392405}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d63f4c-ca3a-42d4-87d1-8333d5e5b62f",
   "metadata": {},
   "source": [
    "### Using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c735271-0927-4ccd-9455-3adc08d45deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionCNNModel(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FashionCNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the TinyVGG model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels = hidden_units,\n",
    "                kernel_size = 3,\n",
    "                stride = 1,\n",
    "                padding = 1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "model_2 = FashionCNNModel(input_shape=1, \n",
    "    hidden_units=10, \n",
    "    output_shape=len(class_names)).to(device)\n",
    "model_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d508a4-d428-49ae-a7be-e98e0c9c10e0",
   "metadata": {},
   "source": [
    "### Shapes\n",
    "input image is 28 x 28\n",
    "Maxpool with kernel 2 divides by /2\n",
    "28 /2 /2 = 7\n",
    "output features are 7x7\n",
    "7x7 x hidden_units\n",
    "\n",
    "you can also run a test to print how the shapes change during a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3997d9a7-360e-4bf4-a263-cf183f018580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_image_tensor = torch.randn(size=(1,28,28))\n",
    "rand_image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "580724d2-76f6-4a0c-8016-c69a41157f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0548,  0.0372,  0.0177,  0.0535, -0.0364,  0.0044, -0.0111,  0.0026,\n",
       "          0.0523,  0.0356]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2(rand_image_tensor.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e2f1-d924-4688-a047-0f3c20a6f7be",
   "metadata": {},
   "source": [
    "### Loss fn\n",
    "remember, optimizer has to be set on the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76b93a6b-a2ad-442f-aee7-017bd74ea9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), \n",
    "                             lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a67635-54da-4f54-8323-8b7c8167bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                       | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 1.02214 | Train accuracy: 63.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████▋                                                                                               | 1/3 [00:05<00:10,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.47727 | Test accuracy: 82.94%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.44001 | Train accuracy: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 2/3 [00:10<00:05,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.43489 | Test accuracy: 84.15%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.38001 | Train accuracy: 86.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.40787 | Test accuracy: 85.17%\n",
      "\n",
      "Train time on cuda: 15.131 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model \n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_2, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b00426d0-0453-4ed2-96d5-96285e5f46fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionCNNModel',\n",
       " 'model_loss': 0.40786755084991455,\n",
       " 'model_acc': 85.16613924050633}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model_2 results \n",
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7faf3f2b-6662-48c8-b277-72a054fc26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_loss</th>\n",
       "      <th>model_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FashionModelV0</td>\n",
       "      <td>0.684929</td>\n",
       "      <td>77.225079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FashionCNNModel</td>\n",
       "      <td>0.407868</td>\n",
       "      <td>85.166139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_name  model_loss  model_acc\n",
       "0   FashionModelV0    0.684929  77.225079\n",
       "1  FashionCNNModel    0.407868  85.166139"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "compare_results = pd.DataFrame([model_0_results, model_2_results])\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13ac05f2-65e1-4e46-a3fc-66be7b141777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_loss</th>\n",
       "      <th>model_acc</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FashionModelV0</td>\n",
       "      <td>0.684929</td>\n",
       "      <td>77.225079</td>\n",
       "      <td>13.406853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FashionCNNModel</td>\n",
       "      <td>0.407868</td>\n",
       "      <td>85.166139</td>\n",
       "      <td>15.131266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_name  model_loss  model_acc  training_time\n",
       "0   FashionModelV0    0.684929  77.225079      13.406853\n",
       "1  FashionCNNModel    0.407868  85.166139      15.131266"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add training times to results comparison\n",
    "compare_results[\"training_time\"] = [total_train_time_model_0,\n",
    "                                    total_train_time_model_2]\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1a2656c-afc0-4afb-bdaf-df6a2958aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionCNNModel                          [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 10, 14, 14]           --\n",
       "│    └─Conv2d: 2-1                       [1, 10, 28, 28]           100\n",
       "│    └─ReLU: 2-2                         [1, 10, 28, 28]           --\n",
       "│    └─Conv2d: 2-3                       [1, 10, 28, 28]           910\n",
       "│    └─ReLU: 2-4                         [1, 10, 28, 28]           --\n",
       "│    └─MaxPool2d: 2-5                    [1, 10, 14, 14]           --\n",
       "├─Sequential: 1-2                        [1, 10, 7, 7]             --\n",
       "│    └─Conv2d: 2-6                       [1, 10, 14, 14]           910\n",
       "│    └─ReLU: 2-7                         [1, 10, 14, 14]           --\n",
       "│    └─Conv2d: 2-8                       [1, 10, 14, 14]           910\n",
       "│    └─ReLU: 2-9                         [1, 10, 14, 14]           --\n",
       "│    └─MaxPool2d: 2-10                   [1, 10, 7, 7]             --\n",
       "├─Sequential: 1-3                        [1, 10]                   --\n",
       "│    └─Flatten: 2-11                     [1, 490]                  --\n",
       "│    └─Linear: 2-12                      [1, 10]                   4,910\n",
       "==========================================================================================\n",
       "Total params: 7,740\n",
       "Trainable params: 7,740\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.16\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.19\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "    \n",
    "from torchinfo import summary\n",
    "summary(model_2, input_size=[1, 1, 28, 28]) # do a test pass through of an example input size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_basic_pytorch",
   "language": "python",
   "name": "ml_basic_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
